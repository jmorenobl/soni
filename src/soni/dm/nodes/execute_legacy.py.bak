import sys
from typing import Any

from langgraph.runtime import Runtime
from langgraph.types import interrupt

from soni.compiler.subgraph import build_flow_subgraph
from soni.core.types import DialogueState
from soni.flow.manager import merge_delta
from soni.runtime.context import RuntimeContext

# Response constants (facilitates i18n and testing)
RESPONSE_CHITCHAT_DEFAULT = "I'm here to help!"
RESPONSE_CANCELLED = "Flow cancelled."
RESPONSE_NO_FLOW = "I can help you. What would you like to do?"


def _get_active_flow_name(state: DialogueState) -> str | None:
    """Get the name of the currently active flow."""
    stack = state.get("flow_stack", [])
    if stack:
        return stack[-1]["flow_name"]
    return None


async def execute_node(
    state: DialogueState,
    runtime: Runtime[RuntimeContext],
) -> dict[str, Any]:
    """Execute flows based on NLU commands.

    Supports link (flow transfer) and call (subflow with return).
    Dynamically rebuilds subgraph when active flow changes.
    """
    sys.stderr.write(f"DEBUG_STDERR: execute_node ENTRY. state keys={list(state.keys())}\n")
    sys.stderr.write(f"DEBUG_STDERR: execute_node ENTRY. flow_stack={state.get('flow_stack')}\n")
    flow_manager = runtime.context.flow_manager
    config = runtime.context.config

    # Process NLU commands
    commands = state.get("commands", []) or []
    updates: dict[str, Any] = {}

    for cmd in commands:
        cmd_type = cmd.get("type")
        sys.stderr.write(f"DEBUG_STDERR: execute_node processing command: {cmd_type}\n")

        if cmd_type == "start_flow":
            flow_name = cmd.get("flow_name")
            if flow_name and flow_name in config.flows:
                sys.stderr.write(f"DEBUG_STDERR: execute_node starting flow: {flow_name}\n")
                _, delta = flow_manager.push_flow(state, flow_name)
                merge_delta(updates, delta)
                # Apply to state for subgraph execution
                if delta.flow_stack:
                    state["flow_stack"] = delta.flow_stack
                if delta.flow_slots:
                    state["flow_slots"] = delta.flow_slots
            else:
                sys.stderr.write(f"DEBUG_STDERR: execute_node flow not found: {flow_name}\n")

        elif cmd_type == "chitchat":
            message = cmd.get("message", RESPONSE_CHITCHAT_DEFAULT)
            return {"response": message, "commands": []}

        elif cmd_type == "set_slot":
            slot = cmd.get("slot")
            value = cmd.get("value")
            if slot:
                sys.stderr.write(f"DEBUG_STDERR: execute_node setting slot: {slot}={value}\n")
                delta = flow_manager.set_slot(state, slot, value)
                if delta:
                    merge_delta(updates, delta)
                    # Apply to state for subgraph execution
                    if delta.flow_slots:
                        # We need to deep merge to accumulate multiple set_slot calls
                        # Types.py has _merge_flow_slots reducer, we can simulate or just use dict merge
                        # Since we rely on flow_manager returning full slots for the flow or just the delta?
                        # FlowManager.set_slot returns delta with just {flow_id: {slot: value}}
                        # We need to manually update state['flow_slots']
                        from soni.core.types import _merge_flow_slots

                        old_slots = state.get("flow_slots", {})
                        sys.stderr.write(
                            f"DEBUG_STDERR: execute_node merge slots. delta={delta.flow_slots}, old={old_slots}\n"
                        )
                        state["flow_slots"] = _merge_flow_slots(old_slots, delta.flow_slots)
                        sys.stderr.write(
                            f"DEBUG_STDERR: execute_node merged slots: {state['flow_slots']}\n"
                        )

        elif cmd_type == "cancel_flow":
            if state.get("flow_stack"):
                _, delta = flow_manager.pop_flow(state)
                merge_delta(updates, delta)
                return {**updates, "response": RESPONSE_CANCELLED, "commands": []}

    # No flow to execute?
    if not state.get("flow_stack"):
        return {"response": RESPONSE_NO_FLOW, "commands": []}

    # Collect all responses
    responses: list[str] = []
    subgraph_state = state.copy()
    subgraph_state["_need_input"] = False
    subgraph_state["_loop_flag"] = False

    updates: dict[str, Any] = {}

    print(
        f"DEBUG: execute_node START. Stack: {[f['flow_name'] for f in subgraph_state.get('flow_stack', [])]}"
    )

    # Phase 2: Check for pending interrupt (Suspension Phase)
    # Only suspend if we don't have fresh commands to process.
    # If we have commands, it means NLU already processed the user's response to the interrupt.
    if state.get("_pending_interrupt") and not commands:
        print("DEBUG: Processing pending interrupt...")
        prompt = state["_pending_interrupt"]

        # This call SUSPENDS execution
        user_response = interrupt(prompt)

        # Phase 3: Resume (Post-Interrupt Phase)
        print("DEBUG: Resumed from interrupt.")
        message = (
            user_response if isinstance(user_response, str) else user_response.get("message", "")
        )
        history = subgraph_state.get("messages", [])

        # NLU
        # Note: We need to use runtime context.
        # Ensure we have valid context.
        nlu_output = await runtime.context.du.acall(
            message, context=runtime.context, history=history
        )

        # Updates for next loop
        updates["commands"] = [cmd.model_dump() for cmd in nlu_output.commands]
        updates["_pending_interrupt"] = None  # Clear flag
        updates["_loop_flag"] = True  # Loop to consume commands

        # Append input to history
        from langchain_core.messages import HumanMessage

        messages = subgraph_state.get("messages", [])
        messages.append(HumanMessage(content=message))
        updates["messages"] = messages

        return updates

    # If we have commands, clear any pending interrupt as it is resolved
    if commands and state.get("_pending_interrupt"):
        print("DEBUG: Commands present, clearing pending interrupt.")
        subgraph_state["_pending_interrupt"] = None
        updates["_pending_interrupt"] = None

    # Phase 1: Execution Phase (Run Subgraph)
    current_flow_name = _get_active_flow_name(subgraph_state)

    if current_flow_name and current_flow_name in config.flows:
        flow_config = config.flows[current_flow_name]
        subgraph = build_flow_subgraph(flow_config)

        # Helper to debug subgraph
        try:
            graph = subgraph.get_graph()
            sys.stderr.write(f"DEBUG_STDERR: Subgraph nodes: {list(graph.nodes.keys())}\n")
        except Exception as e:
            sys.stderr.write(f"DEBUG_STDERR: Failed to inspect graph: {e}\n")

        # Execute subgraph using ainvoke (simpler execution model)
        try:
            sys.stderr.write(
                f"DEBUG_STDERR: Invoking subgraph with state keys: {list(subgraph_state.keys())}\n"
            )
            # PASS context as kwarg to ensure it populates Runtime properly
            result = await subgraph.ainvoke(
                subgraph_state,
                config={"configurable": {"context": runtime.context}},
                context=runtime.context,
            )
            sys.stderr.write(f"DEBUG_STDERR: Subgraph ainvoke result keys: {list(result.keys())}\n")
        except Exception as e:
            import traceback

            sys.stderr.write(f"DEBUG_STDERR: ainvoke failed: {e}\n")
            traceback.print_exc(file=sys.stderr)
            result = {}

        if result.get("response"):
            sys.stderr.write(f"DEBUG_STDERR: execute_node output response: {result['response']}\n")
        else:
            sys.stderr.write("DEBUG_STDERR: execute_node output response is MISSING\n")

        print(f"DEBUG: Subgraph result keys: {list(result.keys())}")
        print(f"DEBUG: Subgraph result _need_input: {result.get('_need_input')}")
        # print(f"DEBUG: Subgraph result flow_slots: {json.dumps(result.get('flow_slots', {}), default=default_serializer)}")

        if result.get("response"):
            responses.append(result["response"])

        # subgrah state update
        subgraph_state.update(result)

        # Handle Outcomes
        if result.get("_need_input"):
            # Transition to Pending Interrupt
            prompt = result["_pending_prompt"]
            print(f"DEBUG: Need input detected. Setting pending interrupt: {prompt}")

            # Flush response and STOP looping to return to user
            subgraph_state["_pending_interrupt"] = prompt
            subgraph_state["_loop_flag"] = False  # STOP! Return to user.

            # DO NOT interrupt here. Return to save state (including response).

        elif result.get("_flow_changed"):
            subgraph_state["_flow_changed"] = False
            subgraph_state["_branch_target"] = None
            subgraph_state["_loop_flag"] = True

        elif len(subgraph_state.get("flow_stack", [])) > 1:
            _, delta = flow_manager.pop_flow(subgraph_state)
            if delta.flow_stack:
                subgraph_state["flow_stack"] = delta.flow_stack
            merge_delta(updates, delta)
            subgraph_state["_loop_flag"] = True

        else:
            # Single flow completed
            pass

    else:
        responses.append(RESPONSE_NO_FLOW)

    # Prepare return value (Phase 1)
    from langchain_core.messages import AIMessage

    messages = subgraph_state.get("messages", [])
    if responses:
        final_response = "\n".join(responses)
        messages.append(AIMessage(content=final_response))

    ret_dict = {
        **updates,
        "response": "\n".join(responses) if responses else None,
        "messages": messages,
        "commands": subgraph_state.get("commands", []),
        "flow_slots": subgraph_state.get("flow_slots", {}),
        "_executed_steps": subgraph_state.get("_executed_steps", {}),
        "_loop_flag": subgraph_state.get("_loop_flag", False),
        "_pending_interrupt": subgraph_state.get("_pending_interrupt"),
        "_need_input": subgraph_state.get("_need_input", False),
    }

    return ret_dict
