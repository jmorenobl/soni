"""DSPy modules for Dialogue Understanding."""

import hashlib
import json
import logging
from datetime import datetime

import dspy
from cachetools import TTLCache

from soni.du.models import DialogueContext, NLUOutput
from soni.du.signatures import DialogueUnderstanding

logger = logging.getLogger(__name__)


class SoniDU(dspy.Module):
    """
    Soni Dialogue Understanding module with structured types.

    This module provides:
    - Type-safe async interface for runtime
    - Sync interface for DSPy optimizers
    - Automatic prompt optimization via DSPy
    - Structured Pydantic models throughout
    """

    def __init__(self, cache_size: int = 1000, cache_ttl: int = 300) -> None:
        """Initialize SoniDU module.

        Args:
            cache_size: Maximum number of cached NLU results
            cache_ttl: Time-to-live for cache entries in seconds
        """
        super().__init__()  # CRITICAL: Must call super().__init__()

        # Create predictor with structured signature
        self.predictor = dspy.ChainOfThought(DialogueUnderstanding)

        # Optional caching layer
        self.nlu_cache: TTLCache[str, NLUOutput] = TTLCache(
            maxsize=cache_size,
            ttl=cache_ttl,
        )

    def forward(
        self,
        user_message: str,
        history: dspy.History,
        context: DialogueContext,
        current_datetime: str = "",
    ) -> dspy.Prediction:
        """Sync forward pass for DSPy optimizers.

        Used during optimization/training with MIPROv2, BootstrapFewShot, etc.

        Args:
            user_message: User's input message
            history: Conversation history (dspy.History)
            context: Dialogue context with slots, actions, flows (DialogueContext)
            current_datetime: Current datetime in ISO format

        Returns:
            dspy.Prediction object with result field containing NLUOutput
        """
        return self.predictor(
            user_message=user_message,
            history=history,
            context=context,
            current_datetime=current_datetime,
        )

    async def aforward(
        self,
        user_message: str,
        history: dspy.History,
        context: DialogueContext,
        current_datetime: str = "",
    ) -> dspy.Prediction:
        """Async forward pass for production runtime.

        Called internally by acall(). Uses async LM calls via DSPy's adapter system.

        Args:
            Same as forward()

        Returns:
            dspy.Prediction object with result field containing NLUOutput
        """
        # DSPy's predictor.acall() handles async LM calls
        return await self.predictor.acall(
            user_message=user_message,
            history=history,
            context=context,
            current_datetime=current_datetime,
        )

    async def understand(
        self,
        user_message: str,
        dialogue_context: dict,
    ) -> dict:
        """High-level async interface for NLU (INLUProvider interface).

        This method adapts the dict-based interface expected by understand_node
        to the typed interface of predict().

        Args:
            user_message: User's input message
            dialogue_context: Dict with current_slots, available_actions, etc.

        Returns:
            Dict with message_type, command, slots, confidence, and reasoning
        """
        # Convert history from dialogue_context
        history_messages = dialogue_context.get("history", [])
        history = dspy.History(messages=history_messages)

        # Convert dialogue_context to DialogueContext model
        # Use waiting_for_slot as current_prompted_slot for NLU prioritization
        context = DialogueContext(
            current_slots=dialogue_context.get("current_slots", {}),
            available_actions=dialogue_context.get("available_actions", []),
            available_flows=dialogue_context.get("available_flows", []),
            current_flow=dialogue_context.get("current_flow", "none"),
            expected_slots=dialogue_context.get("expected_slots", []),
            current_prompted_slot=dialogue_context.get("waiting_for_slot"),
        )

        # Call predict and return as dict
        result = await self.predict(user_message, history, context)
        return dict(result.model_dump())

    async def predict(
        self,
        user_message: str,
        history: dspy.History,
        context: DialogueContext,
    ) -> NLUOutput:
        """High-level async prediction method with caching.

        This is the main entry point for runtime NLU calls. Provides:
        - Structured type inputs (dspy.History, DialogueContext)
        - NLUOutput Pydantic model output
        - Automatic caching
        - Internal datetime management

        Args:
            user_message: User's input message
            history: Conversation history (dspy.History)
            context: Dialogue context (DialogueContext)

        Returns:
            NLUOutput with message_type, command, slots, confidence, and reasoning
        """
        # Calculate current datetime (encapsulation principle)
        current_datetime_str = datetime.now().isoformat()

        # Check cache
        cache_key = self._get_cache_key(user_message, history, context)

        if cache_key in self.nlu_cache:
            cached_result = self.nlu_cache[cache_key]
            # Type assertion for mypy
            assert isinstance(cached_result, NLUOutput)
            return cached_result

        # Call via acall() (public async method)
        prediction = await self.acall(
            user_message=user_message,
            history=history,
            context=context,
            current_datetime=current_datetime_str,
        )

        # Extract structured result (no parsing needed!)
        # Type assertion needed because DSPy returns Any
        result = prediction.result
        if not isinstance(result, NLUOutput):
            raise TypeError(f"Expected NLUOutput, got {type(result)}")

        # Cache and return
        self.nlu_cache[cache_key] = result

        return result

    def _get_cache_key(
        self,
        user_message: str,
        history: dspy.History,
        context: DialogueContext,
    ) -> str:
        """Generate cache key from structured inputs."""
        data = {
            "message": user_message,
            "history_length": len(history.messages),
            "context": context.model_dump(),
        }

        json_str = json.dumps(data, sort_keys=True)
        return hashlib.sha256(json_str.encode()).hexdigest()
